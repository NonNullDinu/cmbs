use crate::format::format_file;

use super::ws_path;
use lexer::Lexer;
use log::*;
use std::error::Error;

use serde::{Deserialize, Serialize};

use quote::quote;

mod kinds;
mod lexer;
mod syn_tree_implementation;
mod visitors;

#[derive(Serialize, Deserialize, Debug)]
pub struct ConstToken {
    name: String,
    #[serde(rename = "lowercase")]
    lowercased_name: String,
    kind: String,
    text: String,
}

#[derive(Serialize, Deserialize, Debug)]
pub struct Token {
    name: String,
    #[serde(rename = "lowercase")]
    lowercased_name: String,
    kind: String,
    regex: String,
}

#[derive(Serialize, Deserialize, Debug)]
pub enum Child {
    Single(String, String),
    Multiple(String, String),
    First(String, String),
    Last(String, String),
    Optional(String, String),
}

impl Child {
    fn get_sytax_element_name(&self) -> &String {
        match self {
            Self::Single(s, ..)
            | Self::Multiple(s, ..)
            | Self::First(s, ..)
            | Self::Last(s, ..)
            | Self::Optional(s, ..) => s,
        }
    }

    // fn get_getter_name_fragment(&self) -> &String {
    //     match self {
    //         Self::Single(_, s, ..)
    //         | Self::Multiple(_, s, ..)
    //         | Self::First(_, s, ..)
    //         | Self::Last(_, s, ..)
    //         | Self::Optional(_, s, ..) => s,
    //     }
    // }
}

#[derive(Serialize, Deserialize, Debug)]
pub struct Node {
    name: String,
    #[serde(rename = "lowercase")]
    lowercased_name: String,
    kind: String,
    children: Vec<Child>,
}

#[derive(Serialize, Deserialize, Debug)]
pub struct Grammar {
    const_tokens: Vec<ConstToken>,
    tokens: Vec<Token>,
    nodes: Vec<Node>,
}

pub fn generate_grammar() -> Result<(), Box<dyn Error>> {
    let p = ws_path!("crates" / "leafbuild-syntax" / "src" / "grammar.ron");
    info!("Generating grammar from {:?}", p);

    let contents = std::fs::read_to_string(p)?;

    let grammar = ron::from_str(&contents)?;
    let Grammar {
        ref const_tokens,
        ref tokens,
        nodes: _,
    } = grammar;

    let lexer = Lexer {
        tokens,
        const_tokens,
    };
    // let syn_tree = SynTree { nodes };

    {
        let s = quote! {#lexer}.to_string();
        let path = ws_path!("crates" / "leafbuild-syntax" / "src" / "lexer.rs");
        std::fs::write(
            &path,
            format!("{}\n{}", "// @generated by xtask generate-grammar", s),
        )?;
        info!("Wrote {:?}, now formatting", &path);

        format_file(&path)?;

        info!("Formatted {:?}", path);
    }

    {
        let s = kinds::get_kinds(&grammar).to_string();
        let path = ws_path!("crates" / "leafbuild-syntax" / "src" / "syntax_kind.rs");
        std::fs::write(
            &path,
            format!("{}\n{}", "// @generated by xtask generate-grammar", s),
        )?;
        info!("Wrote {:?}, now formatting", &path);

        format_file(&path)?;

        info!("Formatted {:?}", path);
    }

    {
        let s = syn_tree_implementation::syn_tree_implementation(&grammar).to_string();
        let path =
            ws_path!("crates" / "leafbuild-syntax" / "src" / "syn_tree" / "implementation.rs");
        std::fs::write(
            &path,
            format!("{}\n{}", "// @generated by xtask generate-grammar", s),
        )?;
        info!("Wrote {:?}, now formatting", &path);

        format_file(&path)?;

        info!("Formatted {:?}", path);
    }

    {
        let s = visitors::visitor(&grammar).to_string();
        let path = ws_path!("crates" / "leafbuild-syntax" / "src" / "syn_tree" / "visitor.rs");
        std::fs::write(
            &path,
            format!("{}\n{}", "// @generated by xtask generate-grammar", s),
        )?;
        info!("Wrote {:?}, now formatting", &path);

        format_file(&path)?;

        info!("Formatted {:?}", path);
    }

    Ok(())
}

#[derive(Debug)]
pub struct SynTree {
    nodes: Vec<Node>,
}

fn raw_str_literal(s: &'_ str) -> ::proc_macro2::TokenStream {
    let mut depth = 0;
    let mut cur_depth = None;
    let mut chars = s.chars().peekable();
    loop {
        match (cur_depth, chars.peek()) {
            (_, Some('#')) => {}
            (Some(d), _) => depth = depth.max(d),
            _ => {}
        }
        match (chars.next(), cur_depth) {
            (None, _) => break,
            (Some('"'), _) => cur_depth = Some(0),
            (Some('#'), Some(ref mut d)) => *d += 1,
            (Some(_), _) => cur_depth = None,
        }
    }
    format!(
        "r{0:#^raw_depth$}\"{wrapped}\"{0:#^raw_depth$}",
        "",
        wrapped = s,
        raw_depth = depth + 1,
    )
    .parse()
    .unwrap()
}

enum ElementType {
    Node,
    Token,
}

impl Grammar {
    fn element_type(&self, name: &str) -> Option<ElementType> {
        if self
            .tokens
            .iter()
            .map(|it| &it.name)
            .chain(self.const_tokens.iter().map(|it| &it.name))
            .any(|it| it == name)
        {
            Some(ElementType::Token)
        } else if self.nodes.iter().map(|it| &it.name).any(|it| it == name) {
            Some(ElementType::Node)
        } else {
            None
        }
    }
}
